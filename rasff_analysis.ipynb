{"cells":[{"cell_type":"markdown","metadata":{"id":"naYl89EtePHV"},"source":["# Prediction of Global Food Security Index (GFSI) Using RASFF Portal Data (Learning from Networks 2023-2024)\n","\n","Alberto Battiston (2086522, alberto.battiston.1@studenti.unipd.it) <br>\n","Daniele Moschetta (2087640, daniele.moschetta@studenti.unipd.it) <br>\n","Francesco Visentin (2083245, francesco.visentin.6@studenti.unipd.it)"]},{"cell_type":"markdown","metadata":{},"source":["To code has been developed and tested using Google Colab.\n","\n","Execution over a local Python 3.10+ enviroment is possible but some extra dependecies may be required. (Eg. PyTorch, scikit-learn, NetworkX...)<br>\n","To run locally and install the needed packages uncomment the code below."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Uncomment if running on a local Python 3.10+ enviroment\n","#!pip install -U numpy\n","#!pip install -U pandas\n","#!pip install -U torch\n","#!pip install -U networkx\n","#!pip install -U scikit-learn\n","#!pip install -U matplotlib"]},{"cell_type":"markdown","metadata":{"id":"8dGlcxzk1W1S"},"source":["## 0 - Setup the environment"]},{"cell_type":"markdown","metadata":{"id":"NFWAQNmk0LBg"},"source":["Install Basemap toolkit, needed for plotting the graph, and PyTorchGeometric"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":495},"executionInfo":{"elapsed":11021,"status":"ok","timestamp":1706287688485,"user":{"displayName":"Daniele Moschetta","userId":"05998913540003988899"},"user_tz":-60},"id":"pLP919Jc0LPC","outputId":"93183111-7024-4818-df22-5e0b7969e9cc"},"outputs":[],"source":["!pip install -U basemap\n","!pip install -U torch_geometric"]},{"cell_type":"markdown","metadata":{"id":"BroWl1yw0PrR"},"source":["Import libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7032,"status":"ok","timestamp":1706287695510,"user":{"displayName":"Daniele Moschetta","userId":"05998913540003988899"},"user_tz":-60},"id":"yChU-fA9Bgby"},"outputs":[],"source":["# General\n","import pandas as pd\n","import numpy as np\n","from collections import defaultdict\n","import copy\n","\n","# Graphs\n","import networkx as nx\n","\n","# Plotting\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.basemap import Basemap as Basemap\n","\n","# Machine learning: linear model approach\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_absolute_error, mean_squared_error\n","\n","# Machine learning: MLP and GNN\n","import torch\n","from torch.nn import Linear\n","import torch.nn.functional as F\n","from torch_geometric.nn import GCNConv\n","from torch_geometric.data import Data, Batch\n","from torch_geometric.loader import DataLoader"]},{"cell_type":"markdown","metadata":{"id":"nC_B8fO8fG7v"},"source":["## 1 - Read the datasets and extract the needed information"]},{"cell_type":"markdown","metadata":{},"source":["Set the GithHub repository as the base path to access the datasets directly from Google Colab. If running locally change to the local repo base path"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["base_path = \"https://raw.githubusercontent.com/FrancescoVisentin/RASFF_Analysis_LFN/main\""]},{"cell_type":"markdown","metadata":{"id":"egRN31z75_cv"},"source":["Read the RASFF dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":256,"status":"ok","timestamp":1706287834817,"user":{"displayName":"Daniele Moschetta","userId":"05998913540003988899"},"user_tz":-60},"id":"u2Yu2DTlsDaK","outputId":"c7357837-799f-448d-8aa6-978faca2c9e7"},"outputs":[],"source":["RASFF_data = pd.read_csv(base_path+'/data/RASFF_2012-2022.csv', on_bad_lines = 'skip')\n","RASFF_data.head()"]},{"cell_type":"markdown","metadata":{"id":"9pRAl_zqC50q"},"source":["Read the GFSI scores"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":466,"status":"ok","timestamp":1706287835549,"user":{"displayName":"Daniele Moschetta","userId":"05998913540003988899"},"user_tz":-60},"id":"QAOmvhIHC8Vb","outputId":"05d54dc5-565c-48ce-c7e6-1f99607a2c11"},"outputs":[],"source":["GFSI_scores = pd.read_csv(base_path+'/data/GFSI_2012-2022.csv')\n","GFSI_scores.head()"]},{"cell_type":"markdown","metadata":{"id":"ZsPJClQt6Ci3"},"source":["Read the geographical information of countries (latitude and longitude)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1706287835550,"user":{"displayName":"Daniele Moschetta","userId":"05998913540003988899"},"user_tz":-60},"id":"K84DKeMu4rK6","outputId":"7971d95e-484f-4999-c306-0ae359a531c4"},"outputs":[],"source":["countries_pos = pd.read_csv(base_path+'/data/countries_coordinates.csv', on_bad_lines = 'skip')\n","countries_pos.head()"]},{"cell_type":"markdown","metadata":{"id":"JrdqyJDm7xou"},"source":["Define the basemap object that is used to plot the nodes on a geographical map"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1429,"status":"ok","timestamp":1706287836973,"user":{"displayName":"Daniele Moschetta","userId":"05998913540003988899"},"user_tz":-60},"id":"yWyFjJxm7xJz"},"outputs":[],"source":["basemap = Basemap(projection='merc', llcrnrlat=-80, urcrnrlat=80, llcrnrlon=-180, urcrnrlon=180, lat_ts=0, resolution='l', suppress_ticks=True)"]},{"cell_type":"markdown","metadata":{"id":"9_qBFB5T3E1P"},"source":["Define a function that takes in input the dataset and extracts the information needed to build the graph. In particular, for each country we keep track of the number of times it is appears in the dataset, considering its role and the risk associated to each notification. Geographical information needed for visualization are also extracted.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1706287836973,"user":{"displayName":"Daniele Moschetta","userId":"05998913540003988899"},"user_tz":-60},"id":"lKGpSFoN3kBX"},"outputs":[],"source":["def get_data(RASFF_data, countriesInfo, basemap):\n","    coordinates_dict = {}\n","    notifications_data = []\n","    risk_dict = defaultdict(lambda : {\"undecided\":0, \"serious\":0, \"potentially serious\":0, \"potential risk\":0, \"not serious\":0, \"no risk\":1})\n","    frequency_dict = defaultdict(lambda : {\"origin\":0, \"operator\":0, \"destination\":0})\n","\n","    # Gets info required for the data visualization\n","    for _, row in countries_pos.iterrows():\n","        coordinates_dict[row[\"country\"]] = basemap(row['longitude'], row['latitude'])\n","\n","    # Gets a record of each notification\n","    for _, row in RASFF_data.iterrows():\n","        origins = [orig for orig in str(row['origins']).split(\",\") if orig != \"nan\"]\n","        operators = [oper for oper in str(row['operators']).split(\",\") if oper != \"nan\"]\n","        destinations = [dist for dist in str(row['destinations']).split(\",\") if dist != \"nan\"]\n","        risk = str(row[\"risk\"])\n","\n","        # Save the notification as a tuple\n","        notifications_data.append((origins, operators, destinations))\n","\n","        # Extract the needed info from each notification\n","        for origin in origins:\n","            frequency_dict[origin][\"origin\"] += 1\n","            for operator in operators:\n","                frequency_dict[operator][\"operator\"] += 1\n","                risk_dict[origin][risk] += 1\n","\n","        for operator in operators:\n","            for destination in destinations:\n","                frequency_dict[destination][\"destination\"] += 1\n","                risk_dict[operator][risk] += 1\n","\n","    return notifications_data, coordinates_dict, frequency_dict, risk_dict"]},{"cell_type":"markdown","metadata":{"id":"r793LwKG3znz"},"source":["Define a function that, given the list of notifications and the information extracted above builds the graph.<br>\n","The result is a directed weighted graph."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1706287836974,"user":{"displayName":"Daniele Moschetta","userId":"05998913540003988899"},"user_tz":-60},"id":"zTNxUI5c3yPj"},"outputs":[],"source":["def build_graph(data, risk_dict, weight_fun):\n","\n","    edges = defaultdict(lambda : .0)\n","    for notification in data:\n","        origins, operators, destinations = notification\n","\n","        # Connects origins to operators (many-to-many)\n","        for origin in origins:\n","            for operator in operators:\n","                edges[(origin, operator)] += weight_fun(origin, operators)\n","\n","        # Connects operators to destinations (many-to-many)\n","        for operator in operators:\n","            for destination in destinations:\n","                edges[(operator, destination)] += weight_fun(operator, destinations)\n","\n","    # Build the graph\n","    graph = nx.DiGraph()\n","    for e, w in edges.items():\n","        graph.add_edge(e[0], e[1], weight=w)\n","\n","    return graph"]},{"cell_type":"markdown","metadata":{"id":"LUrZeq1QCVBQ"},"source":["Define function for plotting the graph. The plot will be different depending on the metrics that we specify, in order to highlight the results. If no metric is speicified, the graph is simply plotted as it is."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1706287836974,"user":{"displayName":"Daniele Moschetta","userId":"05998913540003988899"},"user_tz":-60},"id":"-vk95mBWCbzo"},"outputs":[],"source":["PLOT_NODE_SIZE = 50\n","PLOT_LINE_WIDTH = 0.5\n","\n","def plot_graph(basemap: Basemap, graph: nx.DiGraph, coordinates_dict: dict,  metrics_dict: dict = None,\n","              node_color: str = 'red', edge_color: str = 'black', show_edges = False, show_labels = False):\n","\n","    plt.figure(figsize = (10,9))\n","\n","    if metrics_dict:\n","        color_factor = 250/max(metrics_dict.values())\n","        colors = ['#%02x%02x%02x' % (255, 255-int(metrics_dict[node]*color_factor), 0) for node in graph.nodes()]\n","\n","        size_factor = PLOT_NODE_SIZE * 4 / max(metrics_dict.values())\n","        nx.draw_networkx_nodes(graph, coordinates_dict, node_color=colors,  edgecolors='black', linewidths=PLOT_LINE_WIDTH,\n","                               node_size = [metrics_dict[node] * size_factor for node in graph.nodes()])\n","    else:\n","        nx.draw_networkx_nodes(graph, coordinates_dict, node_color = node_color,  edgecolors='black', linewidths=PLOT_LINE_WIDTH,\n","                               node_size=PLOT_NODE_SIZE)\n","\n","    if show_edges:\n","        nx.draw_networkx_edges(graph, coordinates_dict, arrows = True, edge_color=edge_color, width = PLOT_LINE_WIDTH)\n","\n","    if show_labels:\n","        nx.draw_networkx_labels(graph, coordinates_dict, font_size=5, font_color='black')\n","\n","    basemap.drawcountries(linewidth = PLOT_LINE_WIDTH)\n","    basemap.drawcoastlines(linewidth = PLOT_LINE_WIDTH)\n","\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"a1-Zndpif0oT"},"source":["## 2 - Build the graphs and compute the needed metrics"]},{"cell_type":"markdown","metadata":{"id":"uTSVtOMMK1Fj"},"source":["Choose the weight function"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1706287836974,"user":{"displayName":"Daniele Moschetta","userId":"05998913540003988899"},"user_tz":-60},"id":"krEHE-5pK23f"},"outputs":[],"source":["WEIGHT_FUN_ID = 'W3'\n","\n","# Set the weight function\n","match WEIGHT_FUN_ID:\n","    case 'W2':\n","        weight_fun = lambda country, out_countries: risk_dict[country][\"serious\"] / sum(risk_dict[country].values())\n","    case 'W3':\n","        weight_fun = lambda country, out_countries: len(out_countries) * risk_dict[country][\"serious\"] / (sum(risk_dict[country].values()) - risk_dict[country][\"undecided\"])\n","    case _:\n","        weight_fun = lambda country, out_countries: 1"]},{"cell_type":"markdown","metadata":{"id":"IyR1N9Ez7SQB"},"source":["Now, we are ready to call the previously defined functions and actually build the graphs used for our analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4320,"status":"ok","timestamp":1706287841291,"user":{"displayName":"Daniele Moschetta","userId":"05998913540003988899"},"user_tz":-60},"id":"6yDYrb_S7bOH"},"outputs":[],"source":["graphs = []\n","coordinates_dicts = []\n","frequency_dicts = []\n","risk_dicts = []\n","\n","# Convert dates to datetime\n","RASFF_data['date'] = pd.to_datetime(RASFF_data['date'])\n","\n","first_year = RASFF_data['date'].dt.year.min()\n","last_year = RASFF_data['date'].dt.year.max()\n","\n","for year in range(first_year, last_year + 1):\n","    # Filter the records by year\n","    cur_data = RASFF_data[RASFF_data['date'].dt.year == year]\n","\n","    # Extract info from the dataset and returns the data structures used to build graph\n","    notifications_data, coordinates_dict, frequency_dict, risk_dict = get_data(cur_data, countries_pos, basemap)\n","\n","    # Build graph\n","    graph = build_graph(notifications_data, risk_dict, weight_fun)\n","\n","    # Add year data to data lists\n","    graphs.append(graph)\n","    coordinates_dicts.append(coordinates_dict)\n","    frequency_dicts.append(frequency_dict)\n","    risk_dicts.append(risk_dict)"]},{"cell_type":"markdown","metadata":{"id":"l_YCcb4QdCiE"},"source":["Plot the graph nodes and edges"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":789},"executionInfo":{"elapsed":9219,"status":"ok","timestamp":1706287850508,"user":{"displayName":"Daniele Moschetta","userId":"05998913540003988899"},"user_tz":-60},"id":"eF5H5UdEQ3yt","outputId":"64f0a7da-e2e4-462f-e253-10b788c9f47a"},"outputs":[],"source":["# Plot the graph without any metric specified\n","plot_graph(basemap, graphs[0], coordinates_dicts[0], show_edges=True)"]},{"cell_type":"markdown","metadata":{"id":"aLoFu2hxDXRd"},"source":["Define a function that is used for computing the main metrics of the graph:\n","\n","\n","* In-degree\n","* Out-degree\n","* Betwenness centrality\n","* Clustering Coefficient\n","* Origin score\n","* Risk score\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3539,"status":"ok","timestamp":1706287854042,"user":{"displayName":"Daniele Moschetta","userId":"05998913540003988899"},"user_tz":-60},"id":"7p3S3GPvRFRu"},"outputs":[],"source":["graphs_metrics = []\n","\n","for i in range(len(graphs)):\n","    # Compute metrics\n","    in_deg = nx.in_degree_centrality(graphs[i])\n","    out_deg = nx.out_degree_centrality(graphs[i])\n","    bwc = nx.betweenness_centrality(graphs[i], normalized=True, weight=\"weight\")\n","    cc = nx.closeness_centrality(graphs[i].reverse(), distance=\"weight\")\n","\n","    # Combine metrics into a single dict\n","    metrics_dict = {}\n","    for country in in_deg.keys():\n","        origin_score = frequency_dicts[i][country][\"origin\"] / sum(frequency_dicts[i][country].values())\n","        risk_score = (risk_dicts[i][country][\"serious\"]) / sum(risk_dicts[i][country].values())\n","        metrics_dict[country] = [origin_score, risk_score, in_deg[country], out_deg[country], bwc[country], cc[country]]\n","\n","    # Add year metrics to metrics list\n","    graphs_metrics.append(metrics_dict)"]},{"cell_type":"markdown","metadata":{"id":"o5dl1W_4D01e"},"source":["Now, we plot the graph based on the metric that we want to visualize. For the sake of brevity, only two of them are riported but it's sufficient to specify the desired metric to visualize it. The higher the metric value, the more red is the color of the node."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":789},"executionInfo":{"elapsed":956,"status":"ok","timestamp":1706287854986,"user":{"displayName":"Daniele Moschetta","userId":"05998913540003988899"},"user_tz":-60},"id":"axtCGulfE3iS","outputId":"2b9e74df-fe8a-442b-cbea-2cc34e2a32b3"},"outputs":[],"source":["# Plot the In-Degree metric\n","in_deg = nx.in_degree_centrality(graphs[0])\n","plot_graph(basemap, graphs[0], coordinates_dicts[0], metrics_dict=in_deg)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":789},"executionInfo":{"elapsed":914,"status":"ok","timestamp":1706287855897,"user":{"displayName":"Daniele Moschetta","userId":"05998913540003988899"},"user_tz":-60},"id":"SWPfsdumF7pC","outputId":"9d1e4eb6-b97d-40ea-8e37-92f3240dc090"},"outputs":[],"source":["# Plot the Betwennes centrality metric\n","bwc = nx.betweenness_centrality(graphs[0], normalized=True, weight=\"weight\")\n","plot_graph(basemap, graphs[0], coordinates_dicts[0], metrics_dict=bwc)"]},{"cell_type":"markdown","metadata":{"id":"lKnFh4o5gREp"},"source":["## 3 - Linear Model approach"]},{"cell_type":"markdown","metadata":{"id":"Whi1-HZeDBJT"},"source":["Compute the feature vectors"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1706287855897,"user":{"displayName":"Daniele Moschetta","userId":"05998913540003988899"},"user_tz":-60},"id":"e-mPlXTmgXKZ"},"outputs":[],"source":["countries_features = []\n","last_year_index = -1\n","\n","# For each year\n","for i in range(len(graphs_metrics)):\n","    cur_year = first_year + i\n","\n","    # Save index of the first feature of the last year\n","    if cur_year == last_year:\n","        last_year_index = len(countries_features)\n","\n","    # For each country\n","    for country in graphs_metrics[i].keys():\n","        # If there is a correspondence in the scores dataset\n","        if str(cur_year) in GFSI_scores.columns and (GFSI_scores['country'] == country).any():\n","            # Append the features to the feature list\n","            country_score = GFSI_scores.loc[GFSI_scores['country'] == country, str(cur_year)].values[0]\n","            country_features = []\n","            country_features.extend(graphs_metrics[i][country])\n","            country_features.append(country_score)\n","            countries_features.append(country_features)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1706287855897,"user":{"displayName":"Daniele Moschetta","userId":"05998913540003988899"},"user_tz":-60},"id":"0BQWCX1amf40","outputId":"406b4701-a075-4ef1-fbde-7fa115083e5e"},"outputs":[],"source":["countries_features = pd.DataFrame(countries_features)\n","countries_features.head()"]},{"cell_type":"markdown","metadata":{"id":"9lzGWQXZDXFF"},"source":["Normalize and then split train and test"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1706287855897,"user":{"displayName":"Daniele Moschetta","userId":"05998913540003988899"},"user_tz":-60},"id":"crqe4HqReGVo"},"outputs":[],"source":["# Extract X and y from the features\n","X = countries_features.iloc[:, :-1]\n","y = countries_features.iloc[:, -1:]\n","\n","# Normalize feature vectors\n","scaler = MinMaxScaler()\n","X_normalized = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n","\n","# Split in train and test sets\n","X_train = X_normalized[:last_year_index]\n","X_test = X_normalized[last_year_index:]\n","y_train = y[:last_year_index]\n","y_test = y[last_year_index:]"]},{"cell_type":"markdown","metadata":{"id":"HfWuCFhXDa4H"},"source":["Fit linear regression model on train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":74},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1706287855897,"user":{"displayName":"Daniele Moschetta","userId":"05998913540003988899"},"user_tz":-60},"id":"j-rjBCbPgqO4","outputId":"74bec615-98dc-4f63-fa75-d938a1238c7e"},"outputs":[],"source":["# Create linear regression model\n","model = LinearRegression()\n","\n","# Fit model on the train set\n","model.fit(X_train, y_train)"]},{"cell_type":"markdown","metadata":{"id":"3P_xVHmcDfUq"},"source":["Predict labels on test"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1706287855897,"user":{"displayName":"Daniele Moschetta","userId":"05998913540003988899"},"user_tz":-60},"id":"nQeUr8U7gs3F","outputId":"f38cb2e5-5762-4b10-8c32-28005529b7dc"},"outputs":[],"source":["# Predict scores for test set\n","y_pred = model.predict(X_test)\n","\n","# Compute the error\n","print(\"Linear model mean L1 loss: \", mean_absolute_error(y_test, y_pred))\n","print(\"Linear model mean MSE loss: \", mean_squared_error(y_test, y_pred))"]},{"cell_type":"markdown","metadata":{"id":"PlVTACVzYoNn"},"source":["Print predictions comparison"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1706287855897,"user":{"displayName":"Daniele Moschetta","userId":"05998913540003988899"},"user_tz":-60},"id":"8KKjlf_ZYqiG","outputId":"2eb0e30d-39dc-4efc-da72-84d5543d4e44"},"outputs":[],"source":["countries = []\n","model_predictions = []\n","true_labels = []\n","\n","# For each country\n","i = 0\n","for country in graphs_metrics[-1].keys():\n","    # If there is a correspondence in the scores dataset\n","    if str(cur_year) in GFSI_scores.columns and (GFSI_scores['country'] == country).any():\n","        countries.append(country)\n","        model_predictions.append(y_pred[i][0])\n","        true_labels.append(y_test.iloc[i, 0])\n","        i += 1\n","\n","predictions_dict = {'Country': countries, 'Model prediction': model_predictions, 'GSFI score': true_labels}\n","predictions_df = pd.DataFrame(predictions_dict)\n","predictions_df = predictions_df.sort_values(by='Model prediction', ascending=False)\n","predictions_df = predictions_df.reset_index(drop=True)\n","with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.precision', 1,):\n","    display(predictions_df)"]},{"cell_type":"markdown","metadata":{"id":"_BIXm_IMqAff"},"source":["## 4 - MLP approach"]},{"cell_type":"markdown","metadata":{"id":"vdRgJ2vueqyg"},"source":["Firstly we define a function to convert the graph to torch_geometric.data.Data and normalize the features"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1706287855897,"user":{"displayName":"Daniele Moschetta","userId":"05998913540003988899"},"user_tz":-60},"id":"J_YDGp0ueqyl"},"outputs":[],"source":["def graph_to_torch_data(graph: nx.DiGraph, graph_index: int):\n","\n","    year = first_year + graph_index\n","\n","    # Compute node ids, features and labels\n","    node_ids = {}\n","    cur_id = 0\n","    node_features = []\n","    node_labels = []\n","    for country in graph.nodes():\n","        if str(year) in GFSI_scores.columns and (GFSI_scores['country'] == country).any():\n","            # Ids\n","            node_ids[country] = cur_id\n","            cur_id += 1\n","            # Features\n","            node_features.append(graphs_metrics[graph_index][country])\n","            # Label\n","            country_score = GFSI_scores.loc[GFSI_scores['country'] == country, str(year)].values[0]\n","            node_labels.append([country_score])\n","\n","    # Compute edge indexes and attributes\n","    edge_index = []\n","    edge_attr = []\n","    for u,v,w in graph.edges(data=True):\n","        if u in node_ids and v in node_ids:\n","            edge_index.append([node_ids[u], node_ids[v]])\n","            edge_attr.append([w[\"weight\"]])\n","\n","    # Convert to tensors\n","    node_features = torch.tensor(node_features, dtype=torch.float)\n","    node_labels = torch.tensor(node_labels, dtype=torch.float)\n","    edge_index = torch.tensor(edge_index, dtype=torch.long)\n","    edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n","\n","    # Normalization\n","    node_features = F.normalize(node_features)\n","\n","    # Create torch_geometric.data.Data\n","    data = Data(x=node_features, y=node_labels, edge_index=edge_index.t().contiguous(), edge_attr=edge_attr)\n","    data.num_nodes = len(node_features)\n","    data.validate()\n","\n","    return data, node_ids"]},{"cell_type":"markdown","metadata":{"id":"XYU_h8GkvhXa"},"source":["Then two general functions to test our models and print the predicted labels"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1706287855897,"user":{"displayName":"Daniele Moschetta","userId":"05998913540003988899"},"user_tz":-60},"id":"iTSXDmzuf7U6"},"outputs":[],"source":["def test_model(model: torch.nn.Module, data: Data):\n","    model.eval()\n","    with torch.no_grad():\n","        pred = model(data)\n","    return pred"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1706287855898,"user":{"displayName":"Daniele Moschetta","userId":"05998913540003988899"},"user_tz":-60},"id":"ZTrR139MgttW"},"outputs":[],"source":["def print_predictions(pred: list, node_ids: dict):\n","  pred_scores = {}\n","  for i, score in enumerate(pred):\n","      country = [c for c in node_ids if node_ids[c] == i][0]\n","      pred_scores[country] = score[0]\n","\n","  countries = []\n","  model_predictions = []\n","  true_labels = []\n","  for country, score in sorted(pred_scores.items(), key=lambda i : i[1], reverse=True):\n","      countries.append(country)\n","      model_predictions.append(score.item())\n","      true_labels.append(GFSI_scores.loc[GFSI_scores['country'] == country, str(last_year)].values[0])\n","\n","  predictions_dict = {'Country': countries, 'Model prediction': model_predictions, 'GSFI score': true_labels}\n","  predictions_df = pd.DataFrame(predictions_dict)\n","  with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.precision', 1,):\n","    display(predictions_df)"]},{"cell_type":"markdown","metadata":{"id":"Qh--nSzafLB7"},"source":["Now we can define our MLP model and define a function to train it"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1706287855898,"user":{"displayName":"Daniele Moschetta","userId":"05998913540003988899"},"user_tz":-60},"id":"sXAVW2xxR-3c"},"outputs":[],"source":["class MLP(torch.nn.Module):\n","    def __init__(self, input_features, hidden_channels):\n","        super().__init__()\n","        torch.manual_seed(1234567)\n","        self.lin1 = Linear(input_features, hidden_channels)\n","        self.lin2 = Linear(hidden_channels, 1)\n","\n","    def forward(self, data: Data):\n","        x = data.x\n","\n","        x = self.lin1.forward(x)\n","        x = F.relu(x)\n","        x = F.dropout(x, p=0.2, training=self.training)\n","        x = self.lin2.forward(x)\n","        x = 100 * F.sigmoid(x)\n","\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1706287855898,"user":{"displayName":"Daniele Moschetta","userId":"05998913540003988899"},"user_tz":-60},"id":"1bx4y_wOqLBH"},"outputs":[],"source":["def train_MLP(model, train_data: Data, val_data: Data, max_iter=1000, patience=20):\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.02, weight_decay=5e-4)\n","    criterion = torch.nn.MSELoss()\n","\n","    min_val_loss = float('inf')\n","    best_model = copy.deepcopy(model)\n","    no_improv_count = 0\n","\n","    for epoch in range(1, max_iter + 1):\n","        # Training phase\n","        model.train()\n","        optimizer.zero_grad()\n","        out = model(train_data)\n","        loss = criterion(out, train_data.y)\n","        loss.backward()\n","        optimizer.step()\n","        train_loss = loss.item()\n","\n","        # Validation phase\n","        model.eval()\n","        with torch.no_grad():\n","            out = model(val_data)\n","            loss = criterion(out, val_data.y)\n","            val_loss = loss.item()\n","\n","        print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n","\n","        # Early stopping check\n","        if val_loss < min_val_loss:\n","            min_val_loss = val_loss\n","            best_model = copy.deepcopy(model)\n","            no_improv_count = 0\n","        else:\n","            no_improv_count += 1\n","\n","        if no_improv_count >= patience:\n","            print(f'Early stopping at epoch {epoch} due to no improvement in validation loss for {patience} epochs.')\n","            break\n","\n","    return best_model"]},{"cell_type":"markdown","metadata":{"id":"gRvyjPeVqLBI"},"source":["Split train, validation and test data"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":474,"status":"ok","timestamp":1706287856361,"user":{"displayName":"Daniele Moschetta","userId":"05998913540003988899"},"user_tz":-60},"id":"IK2ZHBZHqLBJ"},"outputs":[],"source":["data_list = []\n","\n","# For each graph\n","for i in range(len(graphs)):\n","    data, node_ids = graph_to_torch_data(graphs[i], i)\n","    data_list.append(data)\n","\n","# We need to \"compact\" the data from all the training graphs into a single input feature vectors\n","train_data_list = data_list[:-2]\n","train_data = Batch.from_data_list(train_data_list)\n","\n","val_data = data_list[-2]\n","test_data = data_list[-1]"]},{"cell_type":"markdown","metadata":{"id":"wRqAhYf3qLBJ"},"source":["Create the MLP model and train it"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1716,"status":"ok","timestamp":1706287920952,"user":{"displayName":"Daniele Moschetta","userId":"05998913540003988899"},"user_tz":-60},"id":"pjbemhMvqLBK","outputId":"caa193fa-e01f-42b2-91d0-dbca672362a3"},"outputs":[],"source":["HIDDEN_CHANNLES = 20\n","MAX_EPOCH = 1000\n","EPOCH_PATIENCE = 60\n","\n","model = MLP(data_list[0].num_node_features, HIDDEN_CHANNLES)\n","\n","model = train_MLP(model, train_data, val_data, MAX_EPOCH, EPOCH_PATIENCE)"]},{"cell_type":"markdown","metadata":{"id":"vv1PEPRwqLBL"},"source":["Predict labels on test data, compute the L1 and MSE loss"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1706287920953,"user":{"displayName":"Daniele Moschetta","userId":"05998913540003988899"},"user_tz":-60},"id":"i5L5P-WcqLBL","outputId":"720d6efe-c975-4888-8465-2edf29f97868"},"outputs":[],"source":["# Get the model predictions\n","pred = test_model(model, test_data)\n","\n","# Compute the associated loss\n","criterion = torch.nn.L1Loss()\n","loss = criterion(pred, test_data.y)\n","print(f\"MLP model mean L1 loss: {loss.item():.3f}\")\n","\n","criterion = torch.nn.MSELoss()\n","loss = criterion(pred, test_data.y)\n","print(f\"MLP model mean MSE loss: {loss.item():.3f}\")"]},{"cell_type":"markdown","metadata":{"id":"6yU-sWKVqWBX"},"source":["Print the predicted labels"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1706287857048,"user":{"displayName":"Daniele Moschetta","userId":"05998913540003988899"},"user_tz":-60},"id":"bUd3tiub6NNs","outputId":"e0c32db5-88d2-4cc5-f98e-e421d72a8907"},"outputs":[],"source":["print_predictions(pred, node_ids)"]},{"cell_type":"markdown","metadata":{"id":"m1m_LqKK8ogI"},"source":["## 5 - GNN Approach"]},{"cell_type":"markdown","metadata":{"id":"nFlYLBabSWRn"},"source":["Similarly to before we define the GNN model and a function to train it"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1706287857048,"user":{"displayName":"Daniele Moschetta","userId":"05998913540003988899"},"user_tz":-60},"id":"PuO0mbpg8qiA"},"outputs":[],"source":["class GNN(torch.nn.Module):\n","    def __init__(self, input_features, hidden_channels):\n","        super().__init__()\n","        torch.manual_seed(1234567)\n","        self.conv1 = GCNConv(input_features, hidden_channels)\n","        self.conv2 = GCNConv(hidden_channels, 1)\n","\n","    def forward(self, data: Data):\n","        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n","\n","        x = self.conv1.forward(x, edge_index, edge_weight=edge_attr)\n","        x = F.relu(x)\n","        x = F.dropout(x, p=0.2, training=self.training)\n","        x = self.conv2.forward(x, edge_index, edge_weight=edge_attr)\n","        x = 100 * F.sigmoid(x)\n","\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1706287857048,"user":{"displayName":"Daniele Moschetta","userId":"05998913540003988899"},"user_tz":-60},"id":"uqJJPN3PZNbB"},"outputs":[],"source":["def train_GNN(model, train_loader: DataLoader, val_data: Data, max_iter=1000, patience=20):\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.02, weight_decay=5e-4)\n","    criterion = torch.nn.MSELoss()\n","\n","    min_val_loss = float('inf')\n","    best_model = copy.deepcopy(model)\n","    stable_count = 0\n","\n","    # Training phaseS\n","    for epoch in range(1, max_iter + 1):\n","        model.train()\n","        train_loss_sum = 0\n","        for data in train_loader:\n","            optimizer.zero_grad()\n","            out = model(data)\n","            loss = criterion(out, data.y)\n","            loss.backward()\n","            optimizer.step()\n","            train_loss_sum += loss.item()\n","\n","        train_loss = train_loss_sum / len(train_loader.dataset)\n","\n","        # Validation phase\n","        model.eval()\n","        with torch.no_grad():\n","            out = model(val_data)\n","            loss = criterion(out, val_data.y)\n","            val_loss = loss.item()\n","\n","        print(f'Epoch: {epoch:03d}, Average train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n","\n","        # Early stopping check\n","        if val_loss < min_val_loss:\n","            min_val_loss = val_loss\n","            best_model = copy.deepcopy(model)\n","            no_improv_count = 0\n","        else:\n","            no_improv_count += 1\n","\n","        if no_improv_count >= patience:\n","            print(f'Early stopping at epoch {epoch} due to no improvement in validation loss for {patience} epochs.')\n","            break\n","\n","    return best_model"]},{"cell_type":"markdown","metadata":{"id":"TKSDEJBdDv-5"},"source":["Combine train data into a DataLoader"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1706287857048,"user":{"displayName":"Daniele Moschetta","userId":"05998913540003988899"},"user_tz":-60},"id":"nLeWuUwpYFj5"},"outputs":[],"source":["train_data_loader = DataLoader(train_data_list, batch_size = 1, shuffle=False)"]},{"cell_type":"markdown","metadata":{"id":"b9xjKBxVDyKe"},"source":["Create the GNN model and train it"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2262,"status":"ok","timestamp":1706287859303,"user":{"displayName":"Daniele Moschetta","userId":"05998913540003988899"},"user_tz":-60},"id":"koD6NT7YmN6T","outputId":"141c8722-0a02-4b8e-bd7e-2f28bcdf29aa"},"outputs":[],"source":["HIDDEN_CHANNLES = 20\n","MAX_EPOCH = 1000\n","EPOCH_PATIENCE = 60\n","\n","model = GNN(data_list[0].num_node_features, HIDDEN_CHANNLES)\n","\n","model = train_GNN(model, train_data_loader, val_data, MAX_EPOCH, EPOCH_PATIENCE)"]},{"cell_type":"markdown","metadata":{"id":"vKdWW6MSmPmy"},"source":["Predict labels on test data and compute the L1 and MSE loss"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1706287859303,"user":{"displayName":"Daniele Moschetta","userId":"05998913540003988899"},"user_tz":-60},"id":"vKIAlkIcmPnD","outputId":"5ca07cea-250e-412a-c512-8a601b1c69db"},"outputs":[],"source":["# Get the model predictions\n","pred = test_model(model, test_data)\n","\n","# Compute the associated loss\n","criterion = torch.nn.L1Loss()\n","loss = criterion(pred, test_data.y)\n","print(f\"GNN model mean L1 loss: {loss.item():.3f}\")\n","\n","criterion = torch.nn.MSELoss()\n","loss = criterion(pred, test_data.y)\n","print(f\"GNN model mean MSE loss: {loss.item():.3f}\")"]},{"cell_type":"markdown","metadata":{"id":"xfpRWO_YqnZ_"},"source":["Print the predicted labels"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":503,"status":"ok","timestamp":1706287859803,"user":{"displayName":"Daniele Moschetta","userId":"05998913540003988899"},"user_tz":-60},"id":"KVABpn-r6TV9","outputId":"96833977-28ee-4c6a-9ea0-ff05f9196dd6"},"outputs":[],"source":["print_predictions(pred, node_ids)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
